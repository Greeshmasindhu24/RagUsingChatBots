{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "XMKp9fZWTHRG",
        "outputId": "ee88d552-6714-4564-bc77-222f96c29c42"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after class definition on line 69 (<ipython-input-4-ab219d21cc3d>, line 70)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ab219d21cc3d>\"\u001b[0;36m, line \u001b[0;32m70\u001b[0m\n\u001b[0;31m    import os\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 69\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "!pip install faiss-cpu\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install -U langchain_community\n",
        "!pip install PyPDFLoader\n",
        "!pip install pypdf\n",
        "\n",
        "from langchain_community.chat_models import ChatHuggingFace\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=50):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        self.embedding_model = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "    def load_document(self, file_path):\n",
        "        \"\"\"Load document based on file extension.\"\"\"\n",
        "        try:\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                loader = PyPDFLoader(file_path)\n",
        "                return loader.load()\n",
        "            elif file_path.lower().endswith('.txt'):\n",
        "                loader = TextLoader(file_path)\n",
        "                return loader.load()\n",
        "            else:\n",
        "                print(f\"Unsupported file type: {file_path}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading document {file_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_documents(self, file_paths):\n",
        "        \"\"\"Process multiple documents and create a vector store.\"\"\"\n",
        "        all_docs = []\n",
        "        for file_path in file_paths:\n",
        "            docs = self.load_document(file_path)\n",
        "            if docs:\n",
        "                print(f\"Loaded {len(docs)} pages/sections from {file_path}\")\n",
        "                all_docs.extend(docs)\n",
        "\n",
        "        if not all_docs:\n",
        "            print(\"No documents were successfully loaded.\")\n",
        "            return None\n",
        "\n",
        "        chunks = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"Split into {len(chunks)} chunks for indexing\")\n",
        "\n",
        "\n",
        "        vector_store = FAISS.from_documents(chunks, self.embedding_model)\n",
        "        return vector_store\n",
        "\n",
        "#\n",
        "\n",
        "class RAGChatbot:\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "!pip install faiss-cpu\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install -U langchain_community\n",
        "!pip install PyPDFLoader\n",
        "!pip install pypdf\n",
        "\n",
        "from langchain_community.chat_models import ChatHuggingFace\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=50):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        self.embedding_model = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "    def load_document(self, file_path):\n",
        "        \"\"\"Load document based on file extension.\"\"\"\n",
        "        try:\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                loader = PyPDFLoader(file_path)\n",
        "                return loader.load()\n",
        "            elif file_path.lower().endswith('.txt'):\n",
        "                loader = TextLoader(file_path)\n",
        "                return loader.load()\n",
        "            else:\n",
        "                print(f\"Unsupported file type: {file_path}\")\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading document {file_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_documents(self, file_paths):\n",
        "        \"\"\"Process multiple documents and create a vector store.\"\"\"\n",
        "        all_docs = []\n",
        "        for file_path in file_paths:\n",
        "            docs = self.load_document(file_path)\n",
        "            if docs:\n",
        "                print(f\"Loaded {len(docs)} pages/sections from {file_path}\")\n",
        "                all_docs.extend(docs)\n",
        "\n",
        "        if not all_docs:\n",
        "            print(\"No documents were successfully loaded.\")\n",
        "            return None\n",
        "\n",
        "        chunks = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"Split into {len(chunks)} chunks for indexing\")\n",
        "\n",
        "\n",
        "        vector_store = FAISS.from_documents(chunks, self.embedding_model)\n",
        "        return vector_store\n",
        "\n",
        "#\n",
        "\n",
        "class RAGChatbot:\n",
        "    def __init__(self, vector_store, model_id=\"HuggingFaceH4/zephyr-7b-beta\"):\n",
        "        \"\"\"Initialize the RAG chatbot.\"\"\"\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            output_key=\"answer\"\n",
        "        )\n",
        "        self.vector_store = vector_store\n",
        "        self.llm = HuggingFaceHub(repo_id=model_id, model_kwargs={\"temperature\":0.5, \"max_new_tokens\":512})\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            output_key=\"answer\"\n",
        "        )\n",
        "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            memory=self.memory,\n",
        "            return_source_documents=True,\n",
        "            get_chat_history=lambda h: h   )\n",
        "\n",
        "    def chat(self, query):\n",
        "        \"\"\"Process a user query and return a response.\"\"\"\n",
        "        if not query.strip():\n",
        "            return \"I didn't catch that. Could you please rephrase your question?\"\n",
        "\n",
        "        try:\n",
        "            result = self.qa_chain({\"question\": query})\n",
        "            return result[\"answer\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error during query processing: {e}\")\n",
        "            return \"I encountered an error while processing your question. Please try again.\"\n",
        "def main():\n",
        "    \"\"\"Main function to run the chatbot.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Welcome to Document RAG Chatbot!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    file_paths = []\n",
        "    while True:\n",
        "        file_path = input(\"\\nEnter path to a document (or 'done' to finish adding): \")\n",
        "        if file_path.lower() == 'done':\n",
        "            break\n",
        "        if os.path.exists(file_path):\n",
        "            file_paths.append(file_path)\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "\n",
        "    if not file_paths:\n",
        "        print(\"No valid documents provided. Exiting...\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nProcessing documents...\")\n",
        "    processor = DocumentProcessor()\n",
        "    vector_store = processor.process_documents(file_paths)\n",
        "\n",
        "    if vector_store is None:\n",
        "        print(\"Failed to create vector store. Exiting...\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nInitializing chatbot...\")\n",
        "    chatbot = RAGChatbot(vector_store)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"Chatbot: Hi there! I'm your document assistant. Ask me anything about the documents you've uploaded.\")\n",
        "    print(\"Type 'Hey Bot' to restart or 'Good-Bye' to exit.\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'good-bye':\n",
        "            print(\"\\nChatbot: Thank you for chatting with me. Have a great day!\")\n",
        "            break\n",
        "        elif user_input.lower() == 'hey bot':\n",
        "            print(\"\\nChatbot: Hi there! I'm resetting our conversation. What would you like to know about your documents?\")\n",
        "            chatbot.memory.clear()\n",
        "            continue\n",
        "\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(f\"\\nChatbot: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    output_key=\"answer\"\n",
        "\n",
        "\n",
        "        model_kwargs={\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"repetition_penalty\": 1.1\n",
        "}\n",
        "        self.vector_store = vector_store\n",
        "        self.llm = HuggingFaceHub(repo_id=model_id, model_kwargs={\"temperature\":0.5, \"max_new_tokens\":512})\n",
        "        self.memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            output_key=\"answer\"\n",
        "        )\n",
        "\n",
        "\n",
        "        model_kwargs={\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"repetition_penalty\": 1.1\n",
        "        }\n",
        "\n",
        "\n",
        "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            memory=self.memory,\n",
        "            return_source_documents=True,\n",
        "            get_chat_history=lambda h: h   )\n",
        "\n",
        "    def chat(self, query):\n",
        "        \"\"\"Process a user query and return a response.\"\"\"\n",
        "        if not query.strip():\n",
        "            return \"I didn't catch that. Could you please rephrase your question?\"\n",
        "\n",
        "        try:\n",
        "            result = self.qa_chain({\"question\": query})\n",
        "            return result[\"answer\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error during query processing: {e}\")\n",
        "            return \"I encountered an error while processing your question. Please try again.\"\n",
        "def main():\n",
        "    \"\"\"Main function to run the chatbot.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Welcome to Document RAG Chatbot!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    file_paths = []\n",
        "    while True:\n",
        "        file_path = input(\"\\nEnter path to a document (or 'done' to finish adding): \")\n",
        "        if file_path.lower() == 'done':\n",
        "            break\n",
        "        if os.path.exists(file_path):\n",
        "            file_paths.append(file_path)\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "\n",
        "    if not file_paths:\n",
        "        print(\"No valid documents provided. Exiting...\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nProcessing documents...\")\n",
        "    processor = DocumentProcessor()\n",
        "    vector_store = processor.process_documents(file_paths)\n",
        "\n",
        "    if vector_store is None:\n",
        "        print(\"Failed to create vector store. Exiting...\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nInitializing chatbot...\")\n",
        "    chatbot = RAGChatbot(vector_store)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"Chatbot: Hi there! I'm your document assistant. Ask me anything about the documents you've uploaded.\")\n",
        "    print(\"Type 'Hey Bot' to restart or 'Good-Bye' to exit.\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'good-bye':\n",
        "            print(\"\\nChatbot: Thank you for chatting with me. Have a great day!\")\n",
        "            break\n",
        "        elif user_input.lower() == 'hey bot':\n",
        "            print(\"\\nChatbot: Hi there! I'm resetting our conversation. What would you like to know about your documents?\")\n",
        "            chatbot.memory.clear()\n",
        "            continue\n",
        "\n",
        "        response = chatbot.chat(user_input)\n",
        "        print(f\"\\nChatbot: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnPBAk_ltzM",
        "outputId": "3185362b-7708-4c69-84ab-584a686a06db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}